{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfe0bbd",
   "metadata": {},
   "source": [
    "# **Predictive Analytics: Shinkansen Passenger Satisfaction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72fee5",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542dfab2",
   "metadata": {},
   "source": [
    "**1. Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905039e",
   "metadata": {},
   "source": [
    "**2. Load the training and test data separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a838a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "surveydata_train = pd.read_csv(\"Surveydata_train.csv\")\n",
    "traveldata_train = pd.read_csv(\"Traveldata_train.csv\")\n",
    "#test data\n",
    "surveydata_test = pd.read_csv(\"Surveydata_test.csv\")\n",
    "traveldata_test = pd.read_csv(\"Traveldata_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d366f5c",
   "metadata": {},
   "source": [
    "**3. Understand the data (check for each of the following in both the train and test dataset)**\n",
    "<ol>\n",
    "<li>Check a sample of the data</li>\n",
    "<li>Use the info() and describe() functions for more information</li>\n",
    "<li>Look for the presence of null values in the dataset</li>\n",
    "<li>Look for the presence of bad data or unwanted characters like \"$\" or \"#\" in the numerical columns</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ebf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. Check a sample of the data\n",
    "surveydata_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. Check a sample of the data\n",
    "traveldata_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b5ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B. Use the info() and describe() functions for more information\n",
    "surveydata_train.info()\n",
    "surveydata_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba624b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B. Use the info() and describe() functions for more information\n",
    "traveldata_train.info()\n",
    "traveldata_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C. Look for the presence of null values in the dataset\n",
    "surveydata_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C. Look for the presence of null values in the dataset\n",
    "traveldata_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c30f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D. Look for the presence of bad data or unwanted characters like \"$\" or \"#\" in the numerical columns\n",
    "surveydata_train.describe().columns.astype(str).str.contains(\"($|#)\").any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D. Look for the presence of bad data or unwanted characters like \"$\" or \"#\" in the numerical columns\n",
    "traveldata_train.describe().columns.astype(str).str.contains(\"($|#)\").any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fba3f4",
   "metadata": {},
   "source": [
    "**4. Clean the data**\n",
    "<ol>\n",
    "<li>Treat for missing values in both the train & test set</li>\n",
    "<li>Remove bad data values in both the train & test set</li>\n",
    "<li>Encode the categorical object variables in both the train & test set</li>\n",
    "<li>Perform Feature Engineering if necessary</li>\n",
    "<li>Scale/Normalize the dataset if necessary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a308ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_cleaning(df):\n",
    "    # Before cleaning any data, it is important to transform Categorical values to numerical values\n",
    "    # Retrieve categorical columns, which have data type as \"object\"\n",
    "    df_object_columns = df.select_dtypes(include=['object']).columns\n",
    "    # One-hot encoding for categorical variables\n",
    "    df_encoded = pd.get_dummies(df, columns=df_object_columns)#, dummy_na=True)\n",
    "    #A. Treat for missing values in both the train & test set\n",
    "    imputer = KNNImputer(n_neighbors=15)\n",
    "    #np array is created\n",
    "    df_imputed = imputer.fit_transform(df_encoded)\n",
    "    #back to dataframe\n",
    "    df_without_nans = pd.DataFrame(data=df_imputed, columns=df_encoded.columns)\n",
    "    \n",
    "    return df_without_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be58d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "surveydata_train_clean = dataframe_cleaning(surveydata_train)\n",
    "traveldata_train_clean = dataframe_cleaning(traveldata_train)\n",
    "#test data\n",
    "surveydata_test_clean = dataframe_cleaning(surveydata_test)\n",
    "traveldata_test_clean = dataframe_cleaning(traveldata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52753daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a column is not present on the test set, then it is not important in the train set. Sans the target variable \"Overall_Experience\"\n",
    "def shape_equalizer(df1, df2):\n",
    "    \"\"\"train, then test\"\"\"\n",
    "    df1_columns = df1.columns\n",
    "    df2_columns = df2.columns\n",
    "    difference = list(set(df1_columns).difference(set(df2_columns)))\n",
    "    if \"Overall_Experience\" in difference:\n",
    "        difference.pop(difference.index(\"Overall_Experience\"))\n",
    "    df1 = df1.drop(difference, axis=1)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fbb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#survey data\n",
    "surveydata_train_equalized, surveydata_test_equalized = shape_equalizer(surveydata_train_clean, surveydata_test_clean)\n",
    "#travel data\n",
    "traveldata_train_equalized, traveldata_test_equalized = shape_equalizer(traveldata_train_clean, traveldata_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a83d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "if surveydata_train_equalized.shape[0] == traveldata_train_equalized.shape[0] and surveydata_test_equalized.shape[0] == traveldata_test_equalized.shape[0]:\n",
    "    print(\"Same number of rows between survey and travel data sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 due to target column \"Overall_Experience\"\n",
    "if surveydata_train_equalized.shape[1]-1 == surveydata_test_equalized.shape[1] and traveldata_train_equalized.shape[1] == traveldata_test_equalized.shape[1]:\n",
    "    print(\"Same number of columns between test and train data sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cfd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#last, join the two datasets for train and the two datasets for test\n",
    "#train\n",
    "train_data = traveldata_train_equalized.merge(surveydata_train_equalized, on='ID')\n",
    "#test\n",
    "test_data = traveldata_test_equalized.merge(surveydata_test_equalized, on='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c1803",
   "metadata": {},
   "source": [
    "## try some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df):\n",
    "    df['Delay_per_Distance'] = df['Departure_Delay_in_Mins'] / df['Travel_Distance']\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    df_poly = poly.fit_transform(df[['Age', 'Travel_Distance']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070885f5",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375d2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(bootstrap= False,\n",
    "                             ccp_alpha= 0.0,\n",
    "                             class_weight= None,\n",
    "                             criterion= 'gini',\n",
    "                             max_depth= None,\n",
    "                             max_features= 'auto',\n",
    "                             max_leaf_nodes= None,\n",
    "                             max_samples= None,\n",
    "                             min_impurity_decrease= 0.0,\n",
    "                             min_samples_leaf= 1,\n",
    "                             min_samples_split= 2,\n",
    "                             min_weight_fraction_leaf= 0.0,\n",
    "                             n_estimators= 2500,\n",
    "                             n_jobs= -1,\n",
    "                             oob_score= False,\n",
    "                             random_state= 42,\n",
    "                             verbose= 0,\n",
    "                             warm_start= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0276380",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(objective= 'binary:logistic',\n",
    "                    use_label_encoder= None,\n",
    "                    base_score= None,\n",
    "                    booster= None,\n",
    "                    callbacks= None,\n",
    "                    colsample_bylevel= None,\n",
    "                    colsample_bynode= None,\n",
    "                    colsample_bytree= 0.5,\n",
    "                    early_stopping_rounds= None,\n",
    "                    enable_categorical= False,\n",
    "                    eval_metric= None,\n",
    "                    feature_types= None,\n",
    "                    gamma= 0.25,\n",
    "                    gpu_id= None,\n",
    "                    grow_policy= None,\n",
    "                    importance_type= None,\n",
    "                    interaction_constraints= None,\n",
    "                    learning_rate= 0.09999999999999999,\n",
    "                    max_bin= None,\n",
    "                    max_cat_threshold= None,\n",
    "                    max_cat_to_onehot= None,\n",
    "                    max_delta_step= None,\n",
    "                    max_depth= 15,\n",
    "                    max_leaves= None,\n",
    "                    min_child_weight= 2,\n",
    "                    monotone_constraints= None,\n",
    "                    n_estimators= 2000,\n",
    "                    n_jobs= -1,\n",
    "                    num_parallel_tree= None,\n",
    "                    predictor= None,\n",
    "                    random_state= 42,\n",
    "                    reg_alpha= 0.1,\n",
    "                    reg_lambda= 0.2,\n",
    "                    sampling_method= None,\n",
    "                    scale_pos_weight= None,\n",
    "                    subsample= 0.9,\n",
    "                    tree_method= None,\n",
    "                    validate_parameters= None,\n",
    "                    verbosity= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = ExtraTreesClassifier(bootstrap= False,\n",
    "                           ccp_alpha= 0.0,\n",
    "                           class_weight= None,\n",
    "                           criterion= 'entropy',\n",
    "                           max_depth= None,\n",
    "                           max_features= 'auto',\n",
    "                           max_leaf_nodes= None,\n",
    "                           max_samples= None,\n",
    "                           min_impurity_decrease= 0.0,\n",
    "                           min_samples_leaf= 1,\n",
    "                           min_samples_split= 5,\n",
    "                           min_weight_fraction_leaf= 0.0,\n",
    "                           n_estimators= 2000,\n",
    "                           n_jobs= -1,\n",
    "                           oob_score= False,\n",
    "                           random_state= 42,\n",
    "                           verbose= 0,\n",
    "                           warm_start= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    lgb.LGBMClassifier(objective='binary', \n",
    "                       boosting_type='gbdt',\n",
    "                       n_estimators=3000, \n",
    "                       learning_rate=0.3,\n",
    "                       subsample_for_bin=200, # default 200,000\n",
    "                       n_jobs=-1,\n",
    "                       max_depth=-1,\n",
    "                       random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save function for all\n",
    "def results_to_csv(y_pred, var_name):\n",
    "    data = test_data.copy()\n",
    "    data['Overall_Experience'] = y_pred\n",
    "    result = data[['ID', 'Overall_Experience']]\n",
    "    #to integers\n",
    "    result[['ID', 'Overall_Experience']] = result[['ID', 'Overall_Experience']].astype(int)\n",
    "    #print head\n",
    "    print(result.head(5))\n",
    "    #save as csv\n",
    "    name = \"_\".join(var_name.split('_')[-2:]) + '_result.csv'\n",
    "    return result.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6c419",
   "metadata": {},
   "source": [
    "#### something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Standardize the features to have mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create pipelines\n",
    "#svm_pipeline = make_pipeline(scaler, svm)\n",
    "rfc_pipeline = make_pipeline(scaler, rfc)\n",
    "xgb_pipeline = make_pipeline(scaler, xgb)\n",
    "ext_pipeline = make_pipeline(scaler, ext)\n",
    "lgbm_pipeline = make_pipeline(scaler, lgbm)\n",
    "\n",
    "# Stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_models = list()\n",
    "base_models.append(('rfc', RandomForestClassifier()))\n",
    "base_models.append(('xgb', XGBClassifier()))\n",
    "#base_models.append(('ext', ExtraTreesClassifier()))\n",
    "base_models.append(('lgbm', make_pipeline(StandardScaler(), lgb.LGBMClassifier())))\n",
    "\n",
    "\n",
    "meta_learner = XGBClassifier(n_estimators=2000, random_state=42, use_label_encoder=False)\n",
    "\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_learner, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "# Define the ensemble model\n",
    "ensemble = VotingClassifier(estimators=[('Random Forest', rfc_pipeline),\n",
    "                                        ('XGBoost', xgb_pipeline),\n",
    "                                        #('ExtraTree', ext_pipeline),\n",
    "                                        #('LightGBM', lgbm_pipeline),\n",
    "                                        ('Stacked',stacked_model)], voting='soft')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13654bba",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = feature_eng(train_data)\n",
    "test_data = feature_eng(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test, split for all\n",
    "X = train_data.drop('Overall_Experience', axis=1)\n",
    "y = train_data['Overall_Experience']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3318a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the ensemble\n",
    "ensemble.fit(X_train, y_train)\n",
    "#ensemble.fit(X, y) #provides more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "y_pred_train = ensemble.predict(X_train)\n",
    "#y_pred_train = ensemble.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on training data\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22703e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on validation data\n",
    "y_pred_val = ensemble.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "#pretty plot\n",
    "plt.figure(figsize=(2,2))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "# Add labels (depends on your problem)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04639787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[[TN,FP],[FN,TP]] = cm\n",
    "Accuracy = (TN + TP) / (TN + FP + FN + TP)\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "F1_score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "maybe_acc = Accuracy - 0.003\n",
    "target = 0.9571372\n",
    "print(f\"cm:\\n{cm}\\nAccuracy:\\t{Accuracy:.7f}\\nPrecision:\\t{Precision:.7f}\\nRecall:\\t\\t{Recall:.7f}\\nF1_score:\\t{F1_score:.7f}\\n\\nMaybe Acc:\\t{maybe_acc:.7f}\\nTarget:\\t\\t{target:.7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d230e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "y_pred_test = ensemble.predict(test_data)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e604e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_to_csv(y_pred_test, \"y_pred_test_voting_ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68979f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
